{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b85fe7b-cd51-4a14-8079-2a6146d72adf",
   "metadata": {},
   "source": [
    "# CS 224N - Neural Network Prediction of Word Embeddings\n",
    "Grabbing GPT-2 word embeddings and training RoBERTa to learn the correct embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be948f-5392-44a7-b250-597527e3f474",
   "metadata": {},
   "source": [
    "## Setting up PyTorch\n",
    "Using PyTorch on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b013644-fe9c-4c94-bc13-4d28c4a6c617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21809b-a2f7-4c40-8a69-69683a08f207",
   "metadata": {},
   "source": [
    "## Grab GPT-2 and Word Embeddings\n",
    "Look at word embeddings GPT-2 has processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4427bc2a-0605-42b2-9b15-5d46dc31753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "word_embeddings = model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3e0e-26f1-45fd-bd87-e8ec07ec4dec",
   "metadata": {},
   "source": [
    "## Import WinoDict Dataset\n",
    "Used so that we don't have the overlapping definitions from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f506bf97-22e6-4e7d-9339-6c6f814551e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fear', 'advocate', 'large', 'small', 'receive', 'give', 'successful', 'available', 'repeat', 'answer', 'fast', 'slow', 'vindicate', 'crush', 'weak', 'heavy', 'steel', 'styrofoam', 'short', 'tall', 'top', 'bottom', 'good', 'bad', 'anchor', 'level', 'well', 'bad', 'punish', 'rescue', 'empty', 'full', 'nosy', 'indiscreet', 'convince', 'understand', 'young', 'old', 'clean', 'remove', 'short', 'delay', 'trash', 'drawer', 'handy', 'light', 'tall', 'high', 'good', 'famous', 'generous', 'grateful', 'hurt', 'ungrateful', 'win', 'lose', 'go', 'here', 'get', 'lose', 'hot', 'cool', 'impatient', 'cautious', 'woman', 'baby', 'chat', 'bark', 'hungry', 'tasty', 'bottom', 'top', 'remove', 'repair', 'annoyed', 'annoying', 'impressed', 'impressive', 'ill', 'concerned', 'truthful', 'skeptical', 'grip', 'popular', 'have', 'love', 'stand', 'sing', 'read', 'write', 'tide', 'wind', 'disappoint', 'out', 'eager', 'know', 'use', 'reach', 'out', 'go', 'safe', 'dangerous', 'golfer', 'dog', 'work', 'sleep', 'prepare', 'enough', 'travel', 'dead', 'later', 'early', 'thick', 'small', 'kill', 'guard', 'bold', 'nervous', 'open', 'lower', 'year', 'month', 'defeat', 'victorious', 'evidence', 'deer', 'begin', 'appear', 'persistent', 'cooperative', 'stop', 'run', 'butter', 'leftover', 'well', 'unnecessary', 'minority', 'majority', 'leave', 'have', 'coffee', 'ink', 'admire', 'influence', 'wide', 'narrow', 'dowdy', 'great', 'promise', 'order', 'research', 'childhood', 'receive', 'deliver', 'slow', 'quick', 'move', 'take', 'strip', 'gear', 'full', 'hungry', 'over', 'next', 'stretch', 'pat', 'accept', 'share', 'silence', 'concentration', 'sympathetic', 'stern', 'ache', 'dangle', 'arm', 'bassinet', 'furious', 'embarrassed', 'retire', 'cancer', 'compassionate', 'cruel', 'suspect', 'regret', 'lemma', 'cautious', 'race', 'big', 'small', 'never', 'frequently', 'cheap', 'expensive', 'thin', 'wide', 'limited', 'substantial', 'tiny', 'huge', 'short', 'tall', 'low', 'high', 'big', 'small', 'employ', 'unemployed', 'small', 'big', 'think', 'explain', 'big', 'short', 'sell', 'buy', 'contrast', 'similar', 'hard', 'soft', 'bad', 'good', 'inexpensive', 'pricy', 'pass', 'fail', 'cool', 'hot', 'darker', 'light', 'firm', 'soft', 'weak', 'strong', 'flat', 'fat', 'good', 'poor', 'comfort', 'scold', 'colorful', 'plain', 'preparation', 'distribution', 'awkward', 'complete', 'embarrassing', 'quiet', 'faulty', 'sharp', 'heavy', 'light', 'hot', 'cold', 'bitter', 'savory', 'full', 'hungry', 'blind', 'sighted', 'cheap', 'strong', 'mild', 'straight', 'keep', 'let', 'shame', 'anger', 'old', 'new', 'small', 'big', 'low', 'high', 'inherit', 'lose', 'large', 'small', 'say', 'hear', 'deep', 'shallow', 'clean', 'dirty', 'appreciate', 'criticize', 'shallow', 'deep', 'flimsy', 'hard', 'hot', 'soft', 'unemployed', 'work', 'small', 'big', 'smooth', 'rough', 'right', 'wrong', 'attend', 'judge', 'deep', 'little', 'low', 'high', 'tiny', 'huge', 'loose', 'tight', 'eat', 'drink', 'much', 'little', 'loud', 'soft', 'dark', 'light', 'sun', 'snow', 'wither', 'sympathetic', 'small', 'large', 'big', 'small', 'nice', 'nasty', 'small', 'large', 'hard', 'easy', 'viscous', 'fluid', 'first', 'later', 'baby', 'freedom', 'small', 'big', 'wide', 'narrow', 'tough', 'soft', 'coach', 'athlete', 'small', 'big', 'always', 'never', 'capable', 'scared', 'able', 'unable', 'small', 'big', 'cold', 'warm', 'rich', 'poor', 'long', 'short', 'student', 'plumber', 'short', 'long', 'away', 'long', 'on', 'off', 'crash', 'healthy', 'massive', 'small', 'strong', 'soft', 'beat', 'treat', 'likely', 'unlikely', 'neat', 'sloppy', 'small', 'big', 'flexible', 'inflexible', 'clear', 'dark', 'like', 'hate', 'big', 'small', 'filter', 'scrub', 'tiny', 'huge', 'empty', 'full', 'fill', 'lack', 'send', 'receive', 'need', 'have', 'forget', 'remember', 'dry', 'wet', 'tall', 'short', 'cheap', 'permanent', 'leaky', 'seal', 'bad', 'fresh', 'big', 'small', 'ab', 'chest', 'legally', 'illegally', 'lose', 'keep', 'short', 'cancel', 'sunset', 'sunrise', 'bland', 'flavorful', 'carrot', 'hockey', 'dirty', 'clean', 'like', 'hate', 'offer', 'accept', 'itchy', 'comfortable', 'slippery', 'dry', 'long', 'short', 'addictive', 'boring', 'big', 'small', 'sell', 'buy', 'bad', 'well', 'patience', 'aptitude', 'late', 'early', 'materialistic', 'realistic', 'disgusting', 'fine', 'sausage', 'pepperoni', 'hate', 'love', 'fast', 'slow', 'inside', 'outside', 'expect', 'give', 'large', 'small', 'slow', 'fast', 'small', 'gigantic', 'stay', 'flee', 'make', 'eat', 'hot', 'cooler', 'smooth', 'abrasive', 'soft', 'hard', 'unhealthy', 'healthy', 'already', 'never', 'cheap', 'expensive', 'love', 'dislike', 'heavy', 'light', 'useless', 'useful', 'humble', 'arrogant']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "first_set = pd.read_csv(\"winodict/prob1_of_5.csv\")\n",
    "winodict_words = first_set['lemma'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833656e-79c6-4e54-b78c-10c7c114d353",
   "metadata": {},
   "source": [
    "## Look at WordNet Definitions and Words\n",
    "Using online package `wn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e693144-f493-407f-8bc3-c713d088e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def create_wordnet_dataset():\n",
    "    words, embeddings, definitions = [], [], []\n",
    "\n",
    "    for ss in wn.all_synsets():\n",
    "        for lemma in ss.lemmas():\n",
    "            word = lemma.name()\n",
    "            if (word is not None and word not in winodict_words):\n",
    "                tokens = tokenizer.encode(word,add_prefix_space=True)\n",
    "                if (len(tokens) == 1):\n",
    "                    words.append(word)\n",
    "                    definitions.append(ss.definition())\n",
    "                    embeddings.append(word_embeddings[tokens,:])\n",
    "    return words, embeddings, definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64f00f-e123-480c-b8c4-b2e32f6d5a24",
   "metadata": {},
   "source": [
    "## Look at Wordset Dictionary Definitions\n",
    "Using `Wordset`, find all of the dictionary words and their definitions. Right now, starting off with all the words from letter `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d392d63-b221-4898-ade9-11a8e0293805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def create_wordset_dataset():\n",
    "    # Create return arrays\n",
    "    final_embeddings, final_words, final_definitions = [], [], []\n",
    "\n",
    "    # All letters of alphabet, plus all eventual words and definitions\n",
    "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    words = np.array([])\n",
    "    definitions = np.array([])\n",
    "\n",
    "    # Grab all the words\n",
    "    for letter in letters:\n",
    "        # Load in the data for each letter\n",
    "        f = open('dictionary/' + letter + '.json')\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Define empty current words and definitions\n",
    "        curr_words, curr_definitions = [], []\n",
    "        for word in list(data.keys()):\n",
    "            if ('meanings' in data[word]):\n",
    "                curr_words.append(word)\n",
    "                curr_definitions.append(\"\")\n",
    "                for index in range(len(data[word]['meanings'])):\n",
    "                    curr_definitions[-1] += data[word]['meanings'][index]['def'] + \". \"\n",
    "\n",
    "        # Update existing numpy array\n",
    "        words = np.concatenate((words, np.array(curr_words)))\n",
    "        definitions = np.concatenate((definitions, np.array(curr_definitions)))\n",
    "    \n",
    "    # Get all words that have embeddings\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        tokens = tokenizer.encode(word,add_prefix_space=True)\n",
    "        if (len(tokens) == 1):\n",
    "            final_embeddings.append(word_embeddings[tokens,:])\n",
    "            final_words.append(word)\n",
    "            final_definitions.append(definitions[i])\n",
    "            \n",
    "    return final_words, final_embeddings, final_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0f1ac",
   "metadata": {},
   "source": [
    "## Shuffle Dataset\n",
    "Randomize order of words and definitions + embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6f6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_dataset(final_words, final_embeddings, final_definitions):\n",
    "    c = list(zip(final_words, final_definitions, final_embeddings))\n",
    "    random.shuffle(c)\n",
    "    final_words, final_definitions, final_embeddings = zip(*c)\n",
    "    return final_words, final_embeddings, final_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3ea5a-92e8-4680-8076-f060db50a287",
   "metadata": {},
   "source": [
    "## Create the Custom Dataset\n",
    "Helpful for extracting embeddings and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986b1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefinitionAndEmbeddings(Dataset):\n",
    "\n",
    "    def __init__(self, final_words, final_embeddings, final_definitions, tokenizer):\n",
    "        self.words = final_words\n",
    "        self.input = final_definitions\n",
    "        self.labels = final_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_definition = self.input[idx]\n",
    "        \n",
    "        # Tokenized input + padding = length of max_length - 1\n",
    "        tokenized_input = self.tokenizer(curr_definition, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_length - 1)\n",
    "        tokenized_cls = self.tokenizer(\" [CLS]\", return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove first dimension from all the embeddings\n",
    "        tokenized_input['input_ids'] = tokenized_input['input_ids'].squeeze(0)\n",
    "        tokenized_cls['input_ids'] = tokenized_cls['input_ids'].squeeze(0)\n",
    "        curr_embedding = self.labels[idx].squeeze(0)\n",
    "        \n",
    "        # Combine the input + padding + CLS token at the end\n",
    "        tokenized_input['input_ids'] = torch.cat((tokenized_input['input_ids'], tokenized_cls['input_ids']), dim=0)\n",
    "        return {'input': tokenized_input, 'output': curr_embedding, 'word': self.words[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8f196-2f6b-496a-8fc0-a481c43b564e",
   "metadata": {},
   "source": [
    "## Add an Initial `CLS` Embedding\n",
    "Per John Hewitt's blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e925e7be-9c17-4102-a04d-df0605c88b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_initial_cls(tokenizer, model):\n",
    "    # Add CLS token\n",
    "    tokenizer.add_tokens(['[CLS]'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Get all the pre-expansion embeddings\n",
    "    params = model.state_dict()\n",
    "    embeddings = params['transformer.wte.weight']\n",
    "    pre_expansion_embeddings = embeddings[:-1,:]\n",
    "    \n",
    "    # Calculate mean, sigma, n\n",
    "    mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
    "    n = pre_expansion_embeddings.size()[0]\n",
    "    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
    "    \n",
    "    # Calculate the distribution\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            mu, covariance_matrix=1e-5*sigma)\n",
    "    \n",
    "    # Load in the new embedding for the CLS token\n",
    "    new_embeddings = torch.stack(tuple((dist.sample() for _ in range(1))), dim=0)\n",
    "    embeddings[-1:,:] = new_embeddings\n",
    "    params['transformer.wte.weight'][-1:,:] = new_embeddings\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46101dc6-3668-4b35-86c6-0e3fae7166e3",
   "metadata": {},
   "source": [
    "## Construct Train and Test Datasets\n",
    "Call from above to generate from either WordNet or WordSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2b5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create the stuff from existing functions\n",
    "final_words, final_embeddings, final_definitions = create_wordnet_dataset()\n",
    "final_words, final_embeddings, final_definitions = shuffle_dataset(final_words, final_embeddings, final_definitions)\n",
    "\n",
    "# Create the new tokenizer (GPT-2 specific)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "add_initial_cls(tokenizer, model)\n",
    "\n",
    "train_words = final_words[:int(0.9 * len(final_words))]\n",
    "train_definitions = final_definitions[:int(0.9 * len(final_definitions))]\n",
    "train_embeddings = final_embeddings[:int(0.9 * len(final_embeddings))]\n",
    "\n",
    "train_dataset = DefinitionAndEmbeddings(train_words, train_embeddings, train_definitions, tokenizer)\n",
    "\n",
    "test_words = final_words[int(0.9 * len(final_words)):]\n",
    "test_definitions = final_definitions[int(0.9 * len(final_definitions)):]\n",
    "test_embeddings = final_embeddings[int(0.9 * len(final_embeddings)):]\n",
    "\n",
    "test_dataset = DefinitionAndEmbeddings(test_words, test_embeddings, test_definitions, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df417af0-8272-4235-b2fe-c86be30daf42",
   "metadata": {},
   "source": [
    "## Set up Training and Testing `DataLoader`s\n",
    "For use in iterating and processing through batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e0184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 2,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(train_dataset, **train_params)\n",
    "\n",
    "test_params = {'batch_size': 2,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "testing_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa091b4-17e1-4756-aafd-4aec40723bfd",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "Also make sure to save weights after every couple of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d071d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [01:04,  3.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get last hidden state\u001b[39;00m\n\u001b[1;32m     29\u001b[0m last_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;241m511\u001b[39m,:]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1043\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1043\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    878\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    879\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:388\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    387\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 388\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:329\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    332\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:199\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add to GPU\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using GPU\")\n",
    "    model.to('cuda')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "best = float('inf')\n",
    "for i in range(5):\n",
    "    training_running_loss = 0.0\n",
    "    \n",
    "    for j, data in tqdm(enumerate(training_loader, 0)):\n",
    "        # Only optimize after every 10th batch or so -- make training more efficient\n",
    "        if (j % 10 == 0):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        input_ids = data['input']['input_ids'].to('cuda')\n",
    "        outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
    "        \n",
    "        # Get last hidden state\n",
    "        last_hidden = outputs.hidden_states[-1][:,511,:]\n",
    "        \n",
    "        # Get the original embeddings and calculate the loss\n",
    "        orig_embeddings = data['output'].to('cuda')\n",
    "        loss = mse_loss(last_hidden, orig_embeddings)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        training_running_loss += loss.item()\n",
    "    \n",
    "    # Take a step once we get outside the batches\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Printing and saving\n",
    "    print(\"training running loss: \", training_running_loss)\n",
    "    if i % 5 == 0:\n",
    "        model.save_pretrained('weights/wordnetepoch' + str(i+1))\n",
    "\n",
    "    # evaluate on test set after every epoch:\n",
    "    testing_running_loss = 0 \n",
    "\n",
    "    for j, data in tqdm(enumerate(testing_loader, 0)):\n",
    "\n",
    "        input_ids = data['input']['input_ids'].to('cuda')\n",
    "        outputs = model(input_ids=input_ids)\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state[:,0,:]\n",
    "        orig_embeddings = data['output'].to('cuda')\n",
    "        loss = mse_loss(last_hidden, orig_embeddings)\n",
    "        testing_running_loss += loss.item()\n",
    "\n",
    "    if testing_running_loss < best:\n",
    "        best = testing_running_loss\n",
    "        model.save_pretrained('weights/GPT2Wordnet')\n",
    "    print(\"testing running loss: \", testing_running_loss)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac71b88f-cdd6-4e71-bb8e-c2c720c58cbd",
   "metadata": {},
   "source": [
    "## Saving Model Weights\n",
    "Save the final model weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dda5daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('weights/GPT2WordnetFinal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
