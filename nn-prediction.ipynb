{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b85fe7b-cd51-4a14-8079-2a6146d72adf",
   "metadata": {},
   "source": [
    "# CS 224N - Neural Network Prediction of Word Embeddings\n",
    "Grabbing GPT-2 word embeddings and training ROBERTA to learn the correct embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be948f-5392-44a7-b250-597527e3f474",
   "metadata": {},
   "source": [
    "## Setting up PyTorch\n",
    "Using PyTorch on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b013644-fe9c-4c94-bc13-4d28c4a6c617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02fed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21809b-a2f7-4c40-8a69-69683a08f207",
   "metadata": {},
   "source": [
    "## Grab GPT-2 and Word Embeddings\n",
    "Look at word embeddings GPT-2 has processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4427bc2a-0605-42b2-9b15-5d46dc31753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (None)/charset_normalizer (3.0.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "word_embeddings = model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3a0ea7-2388-495a-add4-1577951b14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64f00f-e123-480c-b8c4-b2e32f6d5a24",
   "metadata": {},
   "source": [
    "## Load Dictionary Definitions\n",
    "Using `Wordset`, find all of the dictionary words and their definitions. Right now, starting off with all the words from letter `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d392d63-b221-4898-ade9-11a8e0293805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# All letters of alphabet, plus all eventual words and definitions\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "words = np.array([])\n",
    "definitions = np.array([])\n",
    "\n",
    "# Grab all the words\n",
    "for letter in letters:\n",
    "    # Load in the data for each letter\n",
    "    f = open('dictionary/' + letter + '.json')\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # Define empty current words and definitions\n",
    "    curr_words, curr_definitions = [], []\n",
    "    for word in list(data.keys()):\n",
    "        if ('meanings' in data[word]):\n",
    "            curr_words.append(word)\n",
    "            curr_definitions.append(\"\")\n",
    "            for index in range(len(data[word]['meanings'])):\n",
    "                curr_definitions[-1] += data[word]['meanings'][index]['def'] + \". \"\n",
    "    \n",
    "    # Update existing numpy array\n",
    "    words = np.concatenate((words, np.array(curr_words)))\n",
    "    definitions = np.concatenate((definitions, np.array(curr_definitions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f58a5-d491-429c-9a3e-66a78766f67a",
   "metadata": {},
   "source": [
    "## Get all Words that have Embeddings \n",
    "Tokenize each word, check which are singular tokens, and collect all correspondings words and their definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcd9783-aaab-4cc4-8894-4a9faac71c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings, final_words, final_definitions = [], [], []\n",
    "for i in range(len(words)):\n",
    "    word = words[i]\n",
    "    tokens = tokenizer.encode(word,add_prefix_space=True)\n",
    "    if (len(tokens) == 1):\n",
    "        final_embeddings.append(word_embeddings[tokens,:])\n",
    "        final_words.append(word)\n",
    "        final_definitions.append(definitions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6121bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about\n",
      "tensor([ 3.5818e-03,  6.9158e-02,  2.6934e-02,  3.2574e-02,  1.2125e-03,\n",
      "        -6.7487e-02, -2.6660e-01,  1.1424e-02, -8.1788e-02,  1.1982e-01,\n",
      "         1.1785e-02,  1.5385e-02,  9.6082e-02, -7.0277e-02,  1.7012e-01,\n",
      "        -1.5220e-02, -6.4566e-02, -3.2492e-02,  8.0022e-02,  6.3599e-02,\n",
      "         1.5231e-01, -7.5073e-02, -1.0552e-01,  9.6400e-02,  4.6874e-03,\n",
      "         3.7594e-02, -2.7366e-02,  9.4609e-02,  4.5500e-02, -5.7275e-02,\n",
      "        -5.9990e-02, -4.1737e-02,  4.1300e-02, -1.2985e-02,  1.7531e-01,\n",
      "         1.2025e-01, -3.1544e-01, -1.4215e-01,  4.9248e-03,  4.8260e-02,\n",
      "         8.3238e-02,  2.1300e-02, -1.0378e-01,  1.1654e-01, -4.5384e-02,\n",
      "         6.3146e-03, -1.8227e-01,  6.3809e-02, -5.2265e-02, -7.5506e-02,\n",
      "        -6.4237e-02,  9.1597e-02,  1.0758e-01,  7.3340e-02,  5.0310e-02,\n",
      "        -1.1204e-01,  3.0986e-02, -1.1251e-01, -8.1787e-02,  6.0744e-02,\n",
      "        -1.0306e-01, -2.9245e-02,  7.6112e-02, -1.7006e-02, -1.9620e-01,\n",
      "         6.0342e-02, -4.6941e-03, -4.7242e-02,  1.0891e-01, -5.8209e-03,\n",
      "         6.5178e-02, -6.7015e-02,  9.0537e-02, -8.8661e-02,  1.0136e-02,\n",
      "        -1.1035e-01, -5.9812e-02,  3.3137e-01, -1.4661e-02,  2.6114e-01,\n",
      "        -5.8906e-02,  2.9110e-02,  2.9818e-02, -4.3807e-02,  4.1257e-01,\n",
      "         4.0049e-03, -9.6017e-02, -2.3068e-01,  9.9404e-02,  6.8433e-02,\n",
      "        -5.0447e-02,  7.6101e-03,  1.5413e-01, -1.0210e-02,  3.2715e-02,\n",
      "         9.2475e-02, -2.9087e-02,  7.2096e-02,  6.0421e-02, -4.5312e-02,\n",
      "         4.4339e-02, -1.4344e-02, -1.5088e-01, -1.0731e-01, -4.0359e-02,\n",
      "         4.1286e-02, -1.3888e-01, -2.0068e-01, -9.6161e-02,  5.5398e-02,\n",
      "        -2.2464e-02, -1.1940e-01,  3.6281e-02,  9.6917e-02,  6.1019e-02,\n",
      "         1.1593e-01,  1.7191e-02, -3.2704e-02, -5.8107e-02,  9.5101e-02,\n",
      "        -1.0979e-01, -2.1849e-02,  8.3600e-02, -2.0818e-03,  6.4908e-02,\n",
      "         2.6783e-02,  1.3938e-01, -9.4014e-02, -3.4620e-03,  9.4741e-03,\n",
      "        -5.4890e-02, -2.2576e-01,  1.1153e-01,  5.2840e-02,  1.0292e-02,\n",
      "         3.0948e-02,  9.9350e-02, -4.6752e-02,  3.5597e-01, -2.4623e-01,\n",
      "         4.7267e-03, -1.2305e-01,  2.4666e-01, -8.2028e-02, -5.6441e-02,\n",
      "        -4.0936e-01, -1.2592e-01, -5.6029e-02,  3.9766e-02, -3.5529e-02,\n",
      "         1.9965e-01,  2.0374e-02, -7.8470e-03, -6.1407e-02, -1.3151e-01,\n",
      "         3.5435e-02,  8.9634e-02, -1.6307e-02,  1.2795e-01, -1.6794e-01,\n",
      "        -9.8607e-02, -1.7811e-01, -1.0544e-01,  1.0513e-02, -3.2424e-02,\n",
      "        -3.9736e-02, -4.0892e-02,  1.5196e-01,  1.0018e-02,  9.3618e-02,\n",
      "        -2.8728e-02, -5.2167e-02, -1.5743e-01,  6.0619e-02,  4.3356e-03,\n",
      "         7.8098e-02,  7.4571e-02,  2.6414e-02, -8.9174e-03,  1.6141e-02,\n",
      "        -3.4477e-02,  8.8839e-02,  4.4404e-02,  5.0488e-02, -7.2808e-02,\n",
      "        -1.0488e-01, -1.4867e-01,  9.7457e-02, -1.4702e-01, -9.1774e-03,\n",
      "         1.5725e-02,  1.5161e-01, -6.0850e-02,  6.3765e-02,  1.3623e-01,\n",
      "         1.8926e-01,  5.7531e-02, -2.5117e-02, -1.0520e-01,  5.1960e-02,\n",
      "         2.3506e-02, -4.3222e-02,  3.0735e-02, -1.4796e-01, -6.8373e-02,\n",
      "         6.0186e-03, -3.4537e-02, -9.6690e-02,  8.8278e-02,  8.9487e-02,\n",
      "        -1.2638e-01, -6.9011e-02, -9.9292e-02, -3.3843e-02, -1.4502e-01,\n",
      "        -2.5789e-02, -2.3999e-02,  1.4302e-02, -6.6148e-02,  3.5128e-02,\n",
      "        -1.8218e-01, -1.4560e-01,  6.5605e-02,  2.0239e-02, -1.6063e-02,\n",
      "        -7.7470e-02, -4.9563e-03,  6.9662e-02, -8.0511e-02, -1.4243e-01,\n",
      "         3.1857e-02, -7.1227e-02, -2.4865e-01, -5.3282e-02, -1.5253e-01,\n",
      "         3.4279e-02,  9.2802e-02, -3.1252e-03,  4.2131e-02,  4.2264e-02,\n",
      "         4.6234e-02, -8.1997e-03,  6.1711e-02,  1.1333e-01,  1.9354e-01,\n",
      "         9.2595e-02, -3.1599e-03,  3.2998e-02, -8.0468e-02, -3.0026e-02,\n",
      "         5.5001e-02,  8.2520e-02,  7.2048e-02, -1.6432e-01,  1.8604e-01,\n",
      "        -1.9962e-02,  4.2198e-02,  7.4509e-02, -1.1745e-01,  1.7936e-02,\n",
      "         1.2535e-01,  2.0262e-04, -7.8050e-02, -6.4016e-02,  3.3809e-02,\n",
      "        -5.5536e-02, -2.3505e-01,  2.6667e-02,  1.1425e-01, -7.5909e-02,\n",
      "         1.2776e-01, -1.2143e-01, -1.0911e-02,  6.3221e-03,  5.9273e-02,\n",
      "         7.7918e-02,  3.9789e-02, -1.8331e-02,  1.1589e-01,  7.5062e-02,\n",
      "        -1.2094e-01, -2.3689e-02,  9.9105e-03, -4.3449e-02, -8.3790e-05,\n",
      "         2.5071e-02, -7.0113e-02,  7.5382e-02, -5.5053e-02,  1.1166e-01,\n",
      "        -5.0930e-02,  2.6152e-02, -7.0167e-02, -7.1134e-02, -4.6061e-02,\n",
      "         8.6844e-02,  4.0509e-02, -8.0326e-03, -5.1448e-02, -6.4379e-02,\n",
      "         1.1001e-01, -7.3842e-02,  8.8861e-02, -7.8359e-02, -2.8380e-02,\n",
      "         1.4668e-01,  1.1985e-01,  2.9677e-02,  9.3020e-02, -6.2844e-02,\n",
      "        -1.1391e-01,  4.3069e-02,  7.8512e-02,  2.8084e-02,  1.7725e-01,\n",
      "        -8.1503e-02, -1.6645e-01,  4.3775e-03, -1.6214e-01, -2.1147e-01,\n",
      "         2.3396e-01,  4.5448e-03,  1.6697e-01,  2.3928e-02,  7.7145e-02,\n",
      "        -9.6862e-02, -2.3840e-01, -1.6050e-01, -4.7780e-02, -2.0771e-02,\n",
      "        -1.9246e-02,  1.1203e-01, -4.3758e-02,  2.1287e-02,  1.6789e-02,\n",
      "        -3.9967e-02, -3.6531e-02,  5.4516e-02, -6.8807e-03, -1.3022e-01,\n",
      "         1.3643e-01,  1.6767e-01, -1.4265e-01, -8.4035e-02,  1.3611e-02,\n",
      "        -5.7206e-02,  7.4988e-02, -1.1864e-03,  3.4873e-02,  3.3014e-02,\n",
      "        -3.1225e-02, -9.0254e-02,  1.9413e-02, -5.0942e-02, -3.4556e-02,\n",
      "        -1.2568e-01,  1.7592e-02,  2.4748e-02,  2.9256e-01,  4.0344e-02,\n",
      "         1.2076e-01, -7.5586e-03, -2.1631e-01, -1.4315e-01, -1.0653e-01,\n",
      "         1.2337e-01,  5.1664e-02, -4.6505e-02,  1.2393e-01, -9.5968e-02,\n",
      "        -2.5910e-02, -1.2217e-01, -8.1324e-02, -1.7575e-01,  1.7236e-01,\n",
      "        -2.6976e-02,  3.5655e-02, -5.8654e-02, -7.6831e-02,  3.7265e-02,\n",
      "         6.7322e-02, -2.5156e-02,  1.7721e-01,  4.9562e-02, -2.1180e-02,\n",
      "        -1.2985e-01,  5.9586e-02,  1.8555e-02,  4.4538e-02,  2.3987e-01,\n",
      "         6.2188e-02, -3.5309e-02,  7.3614e-02, -1.2706e-01, -1.2346e-02,\n",
      "         1.8104e-01,  2.8861e-02, -4.6511e-02,  1.8968e-02,  9.4440e-02,\n",
      "         1.6582e-01,  5.0699e-02, -8.1472e-02, -4.6408e-02,  2.5425e-02,\n",
      "        -7.2760e-02,  1.9358e-02, -1.3624e-02, -1.4217e-01, -3.9208e-02,\n",
      "        -8.9161e-02,  8.2168e-02,  4.2381e-03,  5.1044e-02,  3.7230e-02,\n",
      "         5.7350e-02,  3.4260e-02, -8.4472e-03,  1.2251e-02, -1.2738e-01,\n",
      "         8.7640e-02,  1.0500e-01,  1.8425e-02, -8.7434e-02,  8.8080e-02,\n",
      "        -1.1247e-01, -4.8936e-02,  3.8899e-02,  2.5454e-02, -1.0655e-01,\n",
      "        -2.5488e-01, -1.3075e-01, -2.1539e-02,  7.5234e-02,  5.8727e-02,\n",
      "        -4.6473e-02, -1.3586e-01,  8.4745e-02,  5.7068e-02, -6.9284e-02,\n",
      "         4.4433e-02,  1.6990e-02, -1.8896e-01,  1.8967e-01, -2.7794e-02,\n",
      "        -4.3162e-02, -1.8919e-02,  1.7443e-01,  4.4563e-02,  4.5938e-02,\n",
      "         2.3525e-02, -6.6979e-03,  2.5992e-01,  4.5033e-03, -1.2534e-01,\n",
      "        -1.0209e-01,  1.5631e-01, -4.4667e-02,  7.9633e-02, -1.6458e-01,\n",
      "         5.0426e-02, -1.3873e-01, -2.0187e-01,  4.8016e-02,  7.1645e-02,\n",
      "        -8.0790e-02, -1.2273e-01,  1.3734e-02, -6.5327e-02,  8.9132e-02,\n",
      "         2.3557e-01,  1.7359e-01, -1.7980e-02, -5.3909e-02, -3.0030e-02,\n",
      "         5.5138e-02, -1.7078e-01, -1.2567e-01,  2.4350e-02,  1.6635e-01,\n",
      "        -2.4838e-01,  1.6233e-01,  9.6888e-02,  1.2505e-01,  9.7526e-02,\n",
      "         2.3176e-02, -4.8335e-02,  1.0106e-01, -7.9747e-02, -1.0879e-01,\n",
      "         6.6489e-02, -2.4774e-02,  2.7861e-02,  5.2774e-02,  7.8627e-02,\n",
      "         1.0278e-01, -2.0459e-01,  6.7281e-02,  5.5869e-02,  1.9092e-01,\n",
      "        -1.0379e-02,  8.3333e-02, -2.6262e-02,  1.5865e-01,  1.9136e-02,\n",
      "         2.7002e-02, -1.3252e-01,  3.3084e-02,  1.0988e-01, -1.5808e-02,\n",
      "        -1.5789e-03,  8.8561e-02, -4.2949e-02,  6.0974e-02, -1.0570e-01,\n",
      "        -8.1560e-02,  2.3929e-02,  2.2318e-02, -7.9541e-02,  8.2143e-02,\n",
      "         9.0981e-02, -7.8649e-02,  3.8292e-02, -8.2759e-02,  8.9784e-02,\n",
      "        -6.4591e-03,  1.5502e-01,  9.2596e-02,  1.0888e-01, -1.1849e-02,\n",
      "        -2.1789e-01,  4.4480e-02,  9.0892e-02, -5.5023e-02, -4.8988e-02,\n",
      "        -1.3756e-01,  4.9073e-02, -9.1996e-02,  2.3771e-02, -3.9504e-02,\n",
      "        -5.8454e-02, -8.5344e-02, -3.9573e-02,  3.0764e-01,  3.6867e-02,\n",
      "         5.3723e-02, -1.3543e-01,  6.3199e-02,  1.2285e-01,  4.7606e-02,\n",
      "         6.9058e-02, -1.9120e-01, -6.3058e-02, -2.9513e-02,  1.4021e-01,\n",
      "         1.3281e-01,  9.6526e-02, -1.1827e-01, -1.5748e-04,  1.2914e-03,\n",
      "        -8.0425e-02, -5.7251e-02,  1.1775e-01, -1.4399e-02,  3.6615e-02,\n",
      "        -8.1575e-02, -2.1078e-02,  2.1561e-01, -6.0326e-02, -6.9509e-02,\n",
      "         8.7675e-03,  2.2030e-02, -1.3069e-01, -1.5505e-01, -6.0865e-02,\n",
      "         1.7488e-02, -3.8482e-02,  7.1060e-02, -8.2567e-03, -1.7181e-02,\n",
      "         1.0467e-01,  5.9807e-02,  6.7409e-02,  4.1789e-02, -7.9215e-02,\n",
      "         1.7188e-01, -9.3581e-02, -1.6454e-01, -1.3718e-01, -9.0929e-02,\n",
      "         1.4017e-01,  1.3375e-01, -8.2894e-02, -2.4556e-02,  8.0328e-02,\n",
      "         2.4493e-02,  5.6055e-02, -1.0560e-01, -9.0061e-02, -4.8355e-02,\n",
      "        -3.7566e-02, -8.9979e-02,  1.2638e-02, -2.0140e-01,  2.6620e-02,\n",
      "        -2.0051e-02, -1.7034e-02,  2.1038e-01, -2.7143e-01, -1.0989e-01,\n",
      "         1.3372e-01,  6.5096e-02,  7.4449e-02,  1.7318e-01,  1.1387e-01,\n",
      "         2.0378e-01,  4.8692e-02, -7.0418e-02,  1.4308e-01, -2.6912e-02,\n",
      "        -4.8204e-02, -3.7128e-02, -7.0598e-02, -3.3254e-02, -3.3260e-02,\n",
      "        -4.5151e-02,  7.3668e-02,  1.5995e-01,  8.1654e-02,  1.8234e-01,\n",
      "        -4.6411e-02,  6.1783e-02, -6.8704e-02, -3.1518e-02,  1.0501e-01,\n",
      "         1.1234e-01, -1.9599e-02, -1.0509e-03,  1.2003e-01,  5.5540e-02,\n",
      "        -1.5697e-01,  8.7938e-02,  7.8850e-02, -6.4285e-04,  2.1505e-02,\n",
      "        -6.0869e-02,  2.8022e-02, -9.3530e-02,  8.3148e-02,  1.0677e-02,\n",
      "        -4.4017e-02, -1.0724e-01, -4.8040e-02, -7.0970e-02, -6.7541e-02,\n",
      "        -4.6114e-02, -1.0169e-01, -8.9439e-02, -1.0924e-01, -1.2608e-01,\n",
      "        -1.0146e-01, -4.2409e-02, -4.7667e-02,  9.1397e-02, -1.3123e-02,\n",
      "        -1.8183e-01,  5.0238e-02, -1.4423e-02, -1.3425e-01, -3.8767e-01,\n",
      "         3.1201e-02,  3.5725e-03,  2.8553e-01,  7.5606e-02, -2.5563e-02,\n",
      "         1.1774e-01, -4.4500e-02, -4.4797e-02, -3.0932e-02, -1.1554e-01,\n",
      "         2.9432e-02,  4.4420e-02, -4.4713e-02,  7.6579e-02,  5.5205e-02,\n",
      "        -6.8921e-02, -5.0275e-02,  6.1668e-02,  8.1651e-02, -1.1518e-01,\n",
      "         5.3327e-02,  1.5641e-03,  5.0392e-02,  1.2827e-01, -7.1521e-02,\n",
      "         7.4870e-02, -1.1237e-02, -1.4043e-01,  5.6265e-02,  6.7615e-02,\n",
      "        -1.2832e-03,  1.4059e-02,  3.2785e-02,  2.0751e-01,  1.4339e-01,\n",
      "        -8.3567e-02,  8.7575e-02,  5.2573e-02, -7.2578e-02,  1.5197e-01,\n",
      "         9.0717e-02,  6.4534e-02, -4.0402e-02, -7.5457e-03, -2.8574e-02,\n",
      "         2.9906e-02, -8.7781e-02,  3.6652e-02, -4.5111e-02, -5.4428e-02,\n",
      "         4.4545e-05,  7.6780e-02, -5.9996e-03, -4.7474e-02, -3.5567e-02,\n",
      "         7.1590e-02, -5.3561e-03, -6.8521e-02,  2.6272e-02,  2.9682e-02,\n",
      "         7.8370e-02, -4.4596e-02, -6.7648e-02, -1.4135e-01, -8.0092e-02,\n",
      "        -8.1149e-02,  1.1195e-01,  7.2475e-02, -7.6483e-02,  1.0561e-01,\n",
      "         7.5440e-02, -6.9464e-02, -6.1428e-02, -1.4130e-01, -1.2541e-02,\n",
      "         7.9760e-02, -1.5249e-02, -2.1068e-01,  5.2362e-02, -1.1787e-02,\n",
      "         4.7198e-02, -7.1105e-03,  8.5806e-02, -1.1317e-02, -6.2054e-02,\n",
      "         9.9607e-02, -3.1001e-02, -6.2147e-02,  4.4033e-02,  3.2395e-02,\n",
      "         1.3586e-02, -5.9855e-02, -1.2240e-01, -5.0652e-02,  9.4063e-02,\n",
      "         7.0388e-02,  2.3668e-02, -2.7131e-02], grad_fn=<SelectBackward0>)\n",
      "imprecise quantities but fairly close to correct. in the area or vicinity. all around or on all sides. in or to a reversed position or direction. used of movement to or among many different places or in no particular direction. (of actions or states) slightly short of or not quite accomplished. in rotation or succession. on the move. \n"
     ]
    }
   ],
   "source": [
    "print(final_words[1])\n",
    "print(final_embeddings[1][0])\n",
    "print(final_definitions[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0f1ac",
   "metadata": {},
   "source": [
    "Now it's time to add in our RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6f6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize order of words and definitions + embeddings\n",
    "import random\n",
    "c = list(zip(final_words, final_definitions, final_embeddings))\n",
    "random.shuffle(c)\n",
    "final_words, final_definitions, final_embeddings = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986b1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a custom pytorch dataset\n",
    "# train_words = 90% of final_words\n",
    "class DefinitionAndEmbeddings(Dataset):\n",
    "\n",
    "    def __init__(self, final_words, final_embeddings, final_definitions, tokenizer):\n",
    "        self.words = final_words\n",
    "        self.input = final_definitions\n",
    "        self.labels = final_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_definition = self.input[idx]\n",
    "        tokenized_input = self.tokenizer(curr_definition, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "        tokenized_input['input_ids'] = tokenized_input['input_ids'].squeeze(0)\n",
    "        curr_embedding = self.labels[idx].squeeze(0)\n",
    "        return {'input': tokenized_input, 'output': curr_embedding, 'word': self.words[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2b5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "train_words = final_words[:int(0.9 * len(final_words))]\n",
    "train_definitions = final_definitions[:int(0.9 * len(final_definitions))]\n",
    "train_embeddings = final_embeddings[:int(0.9 * len(final_embeddings))]\n",
    "\n",
    "train_dataset = DefinitionAndEmbeddings(train_words, train_embeddings, train_definitions, tokenizer)\n",
    "\n",
    "test_words = final_words[int(0.9 * len(final_words)):]\n",
    "test_definitions = final_definitions[int(0.9 * len(final_definitions)):]\n",
    "test_embeddings = final_embeddings[int(0.9 * len(final_embeddings)):]\n",
    "\n",
    "test_dataset = DefinitionAndEmbeddings(test_words, test_embeddings, test_definitions, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e0184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 10,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(train_dataset, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d071d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:05,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:07,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:08,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:10,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:11,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:12,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:14,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:15,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:15,  1.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(orig_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss(last_hidden, orig_embeddings)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=388'>389</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=393'>394</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=394'>395</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py?line=395'>396</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using GPU\")\n",
    "    model.to('cuda')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "for i in range(2):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for j, data in tqdm(enumerate(training_loader, 0)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = data['input']['input_ids'].to('cuda')\n",
    "        outputs = model(input_ids=input_ids)\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state[:,0,:]\n",
    "        print(last_hidden.shape)\n",
    "        orig_embeddings = data['output'].to('cuda')\n",
    "        print(orig_embeddings.shape)\n",
    "        loss = mse_loss(last_hidden, orig_embeddings)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(running_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f90069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1120it [01:49, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.820028885267675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "\n",
    "#for j, data in enumerate(training_loader, 0):\n",
    "#    print(data)\n",
    "#model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "#model = RobertaModel.from_pretrained(\"weights/first_test\")\n",
    "model.to('cuda')\n",
    "for j, data in tqdm(enumerate(training_loader, 0)):\n",
    "\n",
    "    input_ids = data['input']['input_ids'].to('cuda')\n",
    "    outputs = model(input_ids=input_ids)\n",
    "\n",
    "    last_hidden = outputs.last_hidden_state[:,0,:]\n",
    "    #print(last_hidden.shape)\n",
    "    orig_embeddings = data['output'].to('cuda')\n",
    "    #print(orig_embeddings.shape)\n",
    "    loss = mse_loss(last_hidden, orig_embeddings)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "print(running_loss)\n",
    "\n",
    "#print(running_loss / 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('weights/test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {'batch_size': 10,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "testing_loader = DataLoader(test_dataset, **test_params)\n",
    "#test_dataset = DefinitionAndEmbeddings(train_words, train_embeddings, train_definitions, tokenizer)\n",
    "# 14.785202487371862 / 1120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:12, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4687871215865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, data in tqdm(enumerate(testing_loader, 0)):\n",
    "\n",
    "    input_ids = data['input']['input_ids'].to('cuda')\n",
    "    outputs = model(input_ids=input_ids)\n",
    "\n",
    "    last_hidden = outputs.last_hidden_state[:,0,:]\n",
    "    #print(last_hidden.shape)\n",
    "    orig_embeddings = data['output'].to('cuda')\n",
    "    #print(orig_embeddings.shape)\n",
    "    loss = mse_loss(last_hidden, orig_embeddings)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d71e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "125it [00:10, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.53986347001046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "modelBasic = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "modelBasic.to('cuda')\n",
    "for j, data in tqdm(enumerate(testing_loader, 0)):\n",
    "\n",
    "    input_ids = data['input']['input_ids'].to('cuda')\n",
    "    outputs = modelBasic(input_ids=input_ids)\n",
    "\n",
    "    last_hidden = outputs.last_hidden_state[:,0,:]\n",
    "    #print(last_hidden.shape)\n",
    "    orig_embeddings = data['output'].to('cuda')\n",
    "    #print(orig_embeddings.shape)\n",
    "    loss = mse_loss(last_hidden, orig_embeddings)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history > history.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
