{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224N Final Project - Evaluating on Winograd Dataset\n",
    "By: Christopher Pondoc, Joseph Guman, and Joseph O'Brien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \" + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in GPT-2 Model\n",
    "Using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (None)/charset_normalizer (3.0.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Winograd Dataset\n",
    "Also taken from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset winograd_wsc/wsc285 to /home/ubuntu/.cache/huggingface/datasets/winograd_wsc/wsc285/0.0.0/0651311f3b6dda14889d9a063030a02458395ee50ab9f41cca4cd5a89c0c3dce...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46efdb7967b4252be32344150e4a3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/113k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset winograd_wsc downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/winograd_wsc/wsc285/0.0.0/0651311f3b6dda14889d9a063030a02458395ee50ab9f41cca4cd5a89c0c3dce. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a705c1a8f14052828ad3f188e0b305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"winograd_wsc\", 'wsc285')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on One Example\n",
    "Writing a function that is reusable and works for one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_winograd(example):\n",
    "    # First, replace the word with each of the options\n",
    "    first_option, second_option = example['text'], example['text']\n",
    "    first_option = first_option[:example['pronoun_loc']] + example['options'][0] + first_option[example['pronoun_loc'] + len(example['pronoun']):]\n",
    "    second_option = second_option[:example['pronoun_loc']] + example['options'][1] + second_option[example['pronoun_loc'] + len(example['pronoun']):]\n",
    "    \n",
    "    # Tokenize each string and produce labels\n",
    "    first_inputs, second_inputs = tokenizer(first_option, return_tensors=\"pt\"), tokenizer(second_option, return_tensors=\"pt\")\n",
    "    first_labels, second_labels = torch.clone(first_inputs[\"input_ids\"]), torch.clone(second_inputs[\"input_ids\"])\n",
    "    \n",
    "    # Find positioning of tokens of pronoun to split\n",
    "    pronoun = example['pronoun']\n",
    "    start_str, start_ind, end_ind = \"\", -1, -1\n",
    "    original_inputs = tokenizer(example['text'], return_tensors=\"pt\")\n",
    "    for i in range(len(original_inputs[\"input_ids\"][0])):\n",
    "        value = original_inputs[\"input_ids\"][0][i]\n",
    "        if (tokenizer.decode(value).strip()) in pronoun:\n",
    "            start_str += tokenizer.decode(value).strip()\n",
    "            if (start_ind == -1):\n",
    "                start_ind = i\n",
    "            if (start_str == pronoun):\n",
    "                end_ind = i\n",
    "        else:\n",
    "            if (end_ind == -1):\n",
    "                start_ind = -1\n",
    "                start_str = \"\"\n",
    "    \n",
    "    # Create masked string for first option\n",
    "    original_labels = torch.clone(original_inputs[\"input_ids\"])\n",
    "    first_text_tokens = tokenizer(\" \" + example['options'][0], return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    middle_tensor = torch.full((1, len(first_text_tokens)), -100)\n",
    "    final_first_labels = torch.cat((original_labels[:,0:start_ind], middle_tensor, original_labels[:,end_ind + 1:]), dim=1)\n",
    "    \n",
    "    # Create masked string for second option\n",
    "    second_text_tokens = tokenizer(\" \" + example['options'][1], return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    middle_tensor = torch.full((1, len(second_text_tokens)), -100)\n",
    "    final_second_labels = torch.cat((original_labels[:,0:start_ind], middle_tensor, original_labels[:,end_ind + 1:]), dim=1)\n",
    "    \n",
    "    # Evaluate the model on each example and check\n",
    "    first_loss = model(**first_inputs, labels=final_first_labels).loss\n",
    "    second_loss = model(**second_inputs, labels=final_second_labels).loss\n",
    "    \n",
    "    # Write down the correct value and check\n",
    "    if (first_loss < second_loss):\n",
    "        return (example['label'] == 0)\n",
    "    else:\n",
    "        return (example['label'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Winograd on GPT-2\n",
    "Looking specifically at `wsc285`, or the first $285$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Large achieved a score of: 0.6807017543859649\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for example in dataset['test']:\n",
    "    correct += evaluate_winograd(example)\n",
    "    \n",
    "print(\"GPT-2 Large achieved a score of: \" + str((float(correct) / float(len(dataset['test'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
